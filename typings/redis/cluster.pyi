"""
This type stub file was generated by pyright.
"""

from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
from redis._parsers import CommandsParser
from redis.cache import CacheConfig, CacheFactoryInterface, CacheInterface
from redis.client import PubSub, Redis
from redis.commands import RedisClusterCommands
from redis.connection import Connection
from redis.event import EventDispatcher
from redis.retry import Retry
from redis.utils import deprecated_args

def get_node_name(host: str, port: Union[str, int]) -> str:
    ...

@deprecated_args(allowed_args=["redis_node"], reason="Use get_connection(redis_node) instead", version="5.3.0")
def get_connection(redis_node: Redis, *args, **options) -> Connection:
    ...

def parse_scan_result(command, res, **options): # -> tuple[dict[Any, Any], Any | list[Any]]:
    ...

def parse_pubsub_numsub(command, res, **options): # -> list[tuple[Any, Any]]:
    ...

def parse_cluster_slots(resp: Any, **options: Any) -> Dict[Tuple[int, int], Dict[str, Any]]:
    ...

def parse_cluster_shards(resp, **options): # -> list[Any]:
    """
    Parse CLUSTER SHARDS response.
    """
    ...

def parse_cluster_myshardid(resp, **options):
    """
    Parse CLUSTER MYSHARDID response.
    """
    ...

PRIMARY = ...
REPLICA = ...
SLOT_ID = ...
REDIS_ALLOWED_KEYS = ...
KWARGS_DISABLED_KEYS = ...
def cleanup_kwargs(**kwargs): # -> dict[str, Any]:
    """
    Remove unsupported or disabled keys from kwargs
    """
    ...

class AbstractRedisCluster:
    RedisClusterRequestTTL = ...
    PRIMARIES = ...
    REPLICAS = ...
    ALL_NODES = ...
    RANDOM = ...
    DEFAULT_NODE = ...
    NODE_FLAGS = ...
    COMMAND_FLAGS = ...
    SEARCH_COMMANDS = ...
    CLUSTER_COMMANDS_RESPONSE_CALLBACKS = ...
    RESULT_CALLBACKS = ...
    ERRORS_ALLOW_RETRY = ...
    def replace_default_node(self, target_node: ClusterNode = ...) -> None:
        """Replace the default cluster node.
        A random cluster node will be chosen if target_node isn't passed, and primaries
        will be prioritized. The default node will not be changed if there are no other
        nodes in the cluster.

        Args:
            target_node (ClusterNode, optional): Target node to replace the default
            node. Defaults to None.
        """
        ...
    


class RedisCluster(AbstractRedisCluster, RedisClusterCommands):
    @classmethod
    def from_url(cls, url, **kwargs): # -> Self:
        """
        Return a Redis client object configured from the given URL

        For example::

            redis://[[username]:[password]]@localhost:6379/0
            rediss://[[username]:[password]]@localhost:6379/0
            unix://[username@]/path/to/socket.sock?db=0[&password=password]

        Three URL schemes are supported:

        - `redis://` creates a TCP socket connection. See more at:
          <https://www.iana.org/assignments/uri-schemes/prov/redis>
        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:
          <https://www.iana.org/assignments/uri-schemes/prov/rediss>
        - ``unix://``: creates a Unix Domain Socket connection.

        The username, password, hostname, path and all querystring values
        are passed through urllib.parse.unquote in order to replace any
        percent-encoded values with their corresponding characters.

        There are several ways to specify a database number. The first value
        found will be used:

            1. A ``db`` querystring option, e.g. redis://localhost?db=0
            2. If using the redis:// or rediss:// schemes, the path argument
               of the url, e.g. redis://localhost/0
            3. A ``db`` keyword argument to this function.

        If none of these options are specified, the default db=0 is used.

        All querystring options are cast to their appropriate Python types.
        Boolean arguments can be specified with string values "True"/"False"
        or "Yes"/"No". Values that cannot be properly cast cause a
        ``ValueError`` to be raised. Once parsed, the querystring arguments
        and keyword arguments are passed to the ``ConnectionPool``'s
        class initializer. In the case of conflicting arguments, querystring
        arguments always win.

        """
        ...
    
    @deprecated_args(args_to_warn=["read_from_replicas"], reason="Please configure the 'load_balancing_strategy' instead", version="5.3.0")
    @deprecated_args(args_to_warn=["cluster_error_retry_attempts"], reason="Please configure the 'retry' object instead", version="6.0.0")
    def __init__(self, host: Optional[str] = ..., port: int = ..., startup_nodes: Optional[List[ClusterNode]] = ..., cluster_error_retry_attempts: int = ..., retry: Optional[Retry] = ..., require_full_coverage: bool = ..., reinitialize_steps: int = ..., read_from_replicas: bool = ..., load_balancing_strategy: Optional[LoadBalancingStrategy] = ..., dynamic_startup_nodes: bool = ..., url: Optional[str] = ..., address_remap: Optional[Callable[[Tuple[str, int]], Tuple[str, int]]] = ..., cache: Optional[CacheInterface] = ..., cache_config: Optional[CacheConfig] = ..., event_dispatcher: Optional[EventDispatcher] = ..., **kwargs) -> None:
        """
         Initialize a new RedisCluster client.

         :param startup_nodes:
             List of nodes from which initial bootstrapping can be done
         :param host:
             Can be used to point to a startup node
         :param port:
             Can be used to point to a startup node
         :param require_full_coverage:
            When set to False (default value): the client will not require a
            full coverage of the slots. However, if not all slots are covered,
            and at least one node has 'cluster-require-full-coverage' set to
            'yes,' the server will throw a ClusterDownError for some key-based
            commands. See -
            https://redis.io/topics/cluster-tutorial#redis-cluster-configuration-parameters
            When set to True: all slots must be covered to construct the
            cluster client. If not all slots are covered, RedisClusterException
            will be thrown.
        :param read_from_replicas:
             @deprecated - please use load_balancing_strategy instead
             Enable read from replicas in READONLY mode. You can read possibly
             stale data.
             When set to true, read commands will be assigned between the
             primary and its replications in a Round-Robin manner.
        :param load_balancing_strategy:
             Enable read from replicas in READONLY mode and defines the load balancing
             strategy that will be used for cluster node selection.
             The data read from replicas is eventually consistent with the data in primary nodes.
        :param dynamic_startup_nodes:
             Set the RedisCluster's startup nodes to all of the discovered nodes.
             If true (default value), the cluster's discovered nodes will be used to
             determine the cluster nodes-slots mapping in the next topology refresh.
             It will remove the initial passed startup nodes if their endpoints aren't
             listed in the CLUSTER SLOTS output.
             If you use dynamic DNS endpoints for startup nodes but CLUSTER SLOTS lists
             specific IP addresses, it is best to set it to false.
        :param cluster_error_retry_attempts:
             @deprecated - Please configure the 'retry' object instead
             In case 'retry' object is set - this argument is ignored!

             Number of times to retry before raising an error when
             :class:`~.TimeoutError` or :class:`~.ConnectionError`, :class:`~.SlotNotCoveredError` or
             :class:`~.ClusterDownError` are encountered
        :param retry:
            A retry object that defines the retry strategy and the number of
            retries for the cluster client.
            In current implementation for the cluster client (starting form redis-py version 6.0.0)
            the retry object is not yet fully utilized, instead it is used just to determine
            the number of retries for the cluster client.
            In the future releases the retry object will be used to handle the cluster client retries!
        :param reinitialize_steps:
            Specifies the number of MOVED errors that need to occur before
            reinitializing the whole cluster topology. If a MOVED error occurs
            and the cluster does not need to be reinitialized on this current
            error handling, only the MOVED slot will be patched with the
            redirected node.
            To reinitialize the cluster on every MOVED error, set
            reinitialize_steps to 1.
            To avoid reinitializing the cluster on moved errors, set
            reinitialize_steps to 0.
        :param address_remap:
            An optional callable which, when provided with an internal network
            address of a node, e.g. a `(host, port)` tuple, will return the address
            where the node is reachable.  This can be used to map the addresses at
            which the nodes _think_ they are, to addresses at which a client may
            reach them, such as when they sit behind a proxy.

         :**kwargs:
             Extra arguments that will be sent into Redis instance when created
             (See Official redis-py doc for supported kwargs - the only limitation
              is that you can't provide 'retry' object as part of kwargs.
         [https://github.com/andymccurdy/redis-py/blob/master/redis/client.py])
             Some kwargs are not supported and will raise a
             RedisClusterException:
                 - db (Redis do not support database SELECT in cluster mode)
        """
        ...
    
    def __enter__(self): # -> Self:
        ...
    
    def __exit__(self, exc_type, exc_value, traceback): # -> None:
        ...
    
    def __del__(self): # -> None:
        ...
    
    def disconnect_connection_pools(self): # -> None:
        ...
    
    def on_connect(self, connection): # -> None:
        """
        Initialize the connection, authenticate and select a database and send
         READONLY if it is set during object initialization.
        """
        ...
    
    def get_redis_connection(self, node: ClusterNode) -> Redis:
        ...
    
    def get_node(self, host=..., port=..., node_name=...): # -> Redis | None:
        ...
    
    def get_primaries(self): # -> list[Redis]:
        ...
    
    def get_replicas(self): # -> list[Redis]:
        ...
    
    def get_random_node(self): # -> Redis:
        ...
    
    def get_nodes(self): # -> list[Redis]:
        ...
    
    def get_node_from_key(self, key, replica=...): # -> None:
        """
        Get the node that holds the key's slot.
        If replica set to True but the slot doesn't have any replicas, None is
        returned.
        """
        ...
    
    def get_default_node(self): # -> Redis | ClusterNode | None:
        """
        Get the cluster's default node
        """
        ...
    
    def set_default_node(self, node): # -> bool:
        """
        Set the default node of the cluster.
        :param node: 'ClusterNode'
        :return True if the default node was set, else False
        """
        ...
    
    def set_retry(self, retry: Retry) -> None:
        ...
    
    def monitor(self, target_node=...):
        """
        Returns a Monitor object for the specified target node.
        The default cluster node will be selected if no target node was
        specified.
        Monitor is useful for handling the MONITOR command to the redis server.
        next_command() method returns one command from monitor
        listen() method yields commands from monitor.
        """
        ...
    
    def pubsub(self, node=..., host=..., port=..., **kwargs): # -> ClusterPubSub:
        """
        Allows passing a ClusterNode, or host&port, to get a pubsub instance
        connected to the specified node
        """
        ...
    
    def pipeline(self, transaction=..., shard_hint=...): # -> ClusterPipeline:
        """
        Cluster impl:
            Pipelines do not work in cluster mode the same way they
            do in normal mode. Create a clone of this object so
            that simulating pipelines will work correctly. Each
            command will be called directly when used and
            when calling execute() will only return the result stack.
        """
        ...
    
    def lock(self, name, timeout=..., sleep=..., blocking=..., blocking_timeout=..., lock_class=..., thread_local=..., raise_on_release_error: bool = ...): # -> Lock:
        """
        Return a new Lock object using key ``name`` that mimics
        the behavior of threading.Lock.

        If specified, ``timeout`` indicates a maximum life for the lock.
        By default, it will remain locked until release() is called.

        ``sleep`` indicates the amount of time to sleep per loop iteration
        when the lock is in blocking mode and another client is currently
        holding the lock.

        ``blocking`` indicates whether calling ``acquire`` should block until
        the lock has been acquired or to fail immediately, causing ``acquire``
        to return False and the lock not being acquired. Defaults to True.
        Note this value can be overridden by passing a ``blocking``
        argument to ``acquire``.

        ``blocking_timeout`` indicates the maximum amount of time in seconds to
        spend trying to acquire the lock. A value of ``None`` indicates
        continue trying forever. ``blocking_timeout`` can be specified as a
        float or integer, both representing the number of seconds to wait.

        ``lock_class`` forces the specified lock implementation. Note that as
        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is
        a Lua-based lock). So, it's unlikely you'll need this parameter, unless
        you have created your own custom lock class.

        ``thread_local`` indicates whether the lock token is placed in
        thread-local storage. By default, the token is placed in thread local
        storage so that a thread only sees its token, not a token set by
        another thread. Consider the following timeline:

            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.
                     thread-1 sets the token to "abc"
            time: 1, thread-2 blocks trying to acquire `my-lock` using the
                     Lock instance.
            time: 5, thread-1 has not yet completed. redis expires the lock
                     key.
            time: 5, thread-2 acquired `my-lock` now that it's available.
                     thread-2 sets the token to "xyz"
            time: 6, thread-1 finishes its work and calls release(). if the
                     token is *not* stored in thread local storage, then
                     thread-1 would see the token value as "xyz" and would be
                     able to successfully release the thread-2's lock.

        ``raise_on_release_error`` indicates whether to raise an exception when
        the lock is no longer owned when exiting the context manager. By default,
        this is True, meaning an exception will be raised. If False, the warning
        will be logged and the exception will be suppressed.

        In some use cases it's necessary to disable thread local storage. For
        example, if you have code where one thread acquires a lock and passes
        that lock instance to a worker thread to release later. If thread
        local storage isn't disabled in this case, the worker thread won't see
        the token set by the thread that acquired the lock. Our assumption
        is that these cases aren't common and as such default to using
        thread local storage."""
        ...
    
    def set_response_callback(self, command, callback): # -> None:
        """Set a custom Response Callback"""
        ...
    
    def keyslot(self, key): # -> int:
        """
        Calculate keyslot for a given key.
        See Keys distribution model in https://redis.io/topics/cluster-spec
        """
        ...
    
    def determine_slot(self, *args) -> int:
        """
        Figure out what slot to use based on args.

        Raises a RedisClusterException if there's a missing key and we can't
            determine what slots to map the command to; or, if the keys don't
            all map to the same key slot.
        """
        ...
    
    def get_encoder(self): # -> Encoder:
        """
        Get the connections' encoder
        """
        ...
    
    def get_connection_kwargs(self): # -> dict[str, Any]:
        """
        Get the connections' key-word arguments
        """
        ...
    
    def execute_command(self, *args, **kwargs): # -> None:
        ...
    
    def close(self) -> None:
        ...
    
    def load_external_module(self, funcname, func): # -> None:
        """
        This function can be used to add externally defined redis modules,
        and their namespaces to the redis client.

        ``funcname`` - A string containing the name of the function to create
        ``func`` - The function, being added to this class.
        """
        ...
    
    def transaction(self, func, *watches, **kwargs): # -> List[Any]:
        """
        Convenience method for executing the callable `func` as a transaction
        while watching all keys specified in `watches`. The 'func' callable
        should expect a single argument which is a Pipeline object.
        """
        ...
    


class ClusterNode:
    def __init__(self, host, port, server_type=..., redis_connection=...) -> None:
        ...
    
    def __repr__(self): # -> str:
        ...
    
    def __eq__(self, obj) -> bool:
        ...
    
    def __del__(self): # -> None:
        ...
    


class LoadBalancingStrategy(Enum):
    ROUND_ROBIN = ...
    ROUND_ROBIN_REPLICAS = ...
    RANDOM_REPLICA = ...


class LoadBalancer:
    """
    Round-Robin Load Balancing
    """
    def __init__(self, start_index: int = ...) -> None:
        ...
    
    def get_server_index(self, primary: str, list_size: int, load_balancing_strategy: LoadBalancingStrategy = ...) -> int:
        ...
    
    def reset(self) -> None:
        ...
    


class NodesManager:
    def __init__(self, startup_nodes, from_url=..., require_full_coverage=..., lock=..., dynamic_startup_nodes=..., connection_pool_class=..., address_remap: Optional[Callable[[Tuple[str, int]], Tuple[str, int]]] = ..., cache: Optional[CacheInterface] = ..., cache_config: Optional[CacheConfig] = ..., cache_factory: Optional[CacheFactoryInterface] = ..., event_dispatcher: Optional[EventDispatcher] = ..., **kwargs) -> None:
        ...
    
    def get_node(self, host=..., port=..., node_name=...): # -> Redis | None:
        """
        Get the requested node from the cluster's nodes.
        nodes.
        :return: ClusterNode if the node exists, else None
        """
        ...
    
    def update_moved_exception(self, exception): # -> None:
        ...
    
    @deprecated_args(args_to_warn=["server_type"], reason="In case you need select some load balancing strategy " "that will use replicas, please set it through 'load_balancing_strategy'", version="5.3.0")
    def get_node_from_slot(self, slot, read_from_replicas=..., load_balancing_strategy=..., server_type=...) -> ClusterNode:
        """
        Gets a node that servers this hash slot
        """
        ...
    
    def get_nodes_by_server_type(self, server_type): # -> list[Redis]:
        """
        Get all nodes with the specified server type
        :param server_type: 'primary' or 'replica'
        :return: list of ClusterNode
        """
        ...
    
    def populate_startup_nodes(self, nodes): # -> None:
        """
        Populate all startup nodes and filters out any duplicates
        """
        ...
    
    def check_slots_coverage(self, slots_cache): # -> bool:
        ...
    
    def create_redis_connections(self, nodes): # -> None:
        """
        This function will create a redis connection to all nodes in :nodes:
        """
        ...
    
    def create_redis_node(self, host, port, **kwargs): # -> Redis:
        ...
    
    def initialize(self): # -> None:
        """
        Initializes the nodes cache, slots cache and redis connections.
        :startup_nodes:
            Responsible for discovering other nodes in the cluster
        """
        ...
    
    def close(self) -> None:
        ...
    
    def reset(self): # -> None:
        ...
    
    def remap_host_port(self, host: str, port: int) -> Tuple[str, int]:
        """
        Remap the host and port returned from the cluster to a different
        internal value.  Useful if the client is not connecting directly
        to the cluster.
        """
        ...
    
    def find_connection_owner(self, connection: Connection) -> Optional[Redis]:
        ...
    


class ClusterPubSub(PubSub):
    """
    Wrapper for PubSub class.

    IMPORTANT: before using ClusterPubSub, read about the known limitations
    with pubsub in Cluster mode and learn how to workaround them:
    https://redis-py-cluster.readthedocs.io/en/stable/pubsub.html
    """
    def __init__(self, redis_cluster, node=..., host=..., port=..., push_handler_func=..., event_dispatcher: Optional[EventDispatcher] = ..., **kwargs) -> None:
        """
        When a pubsub instance is created without specifying a node, a single
        node will be transparently chosen for the pubsub connection on the
        first command execution. The node will be determined by:
         1. Hashing the channel name in the request to find its keyslot
         2. Selecting a node that handles the keyslot: If read_from_replicas is
            set to true or load_balancing_strategy is set, a replica can be selected.

        :type redis_cluster: RedisCluster
        :type node: ClusterNode
        :type host: str
        :type port: int
        """
        ...
    
    def set_pubsub_node(self, cluster, node=..., host=..., port=...): # -> None:
        """
        The pubsub node will be set according to the passed node, host and port
        When none of the node, host, or port are specified - the node is set
        to None and will be determined by the keyslot of the channel in the
        first command to be executed.
        RedisClusterException will be thrown if the passed node does not exist
        in the cluster.
        If host is passed without port, or vice versa, a DataError will be
        thrown.
        :type cluster: RedisCluster
        :type node: ClusterNode
        :type host: str
        :type port: int
        """
        ...
    
    def get_pubsub_node(self): # -> None:
        """
        Get the node that is being used as the pubsub connection
        """
        ...
    
    def execute_command(self, *args): # -> None:
        """
        Execute a subscribe/unsubscribe command.

        Taken code from redis-py and tweak to make it work within a cluster.
        """
        ...
    
    def get_sharded_message(self, ignore_subscribe_messages=..., timeout=..., target_node=...): # -> None:
        ...
    
    def ssubscribe(self, *args, **kwargs): # -> None:
        ...
    
    def sunsubscribe(self, *args): # -> None:
        ...
    
    def get_redis_connection(self): # -> None:
        """
        Get the Redis connection of the pubsub connected node.
        """
        ...
    
    def disconnect(self): # -> None:
        """
        Disconnect the pubsub connection.
        """
        ...
    


class ClusterPipeline(RedisCluster):
    """
    Support for Redis pipeline
    in cluster mode
    """
    ERRORS_ALLOW_RETRY = ...
    NO_SLOTS_COMMANDS = ...
    IMMEDIATE_EXECUTE_COMMANDS = ...
    UNWATCH_COMMANDS = ...
    @deprecated_args(args_to_warn=["cluster_error_retry_attempts"], reason="Please configure the 'retry' object instead", version="6.0.0")
    def __init__(self, nodes_manager: NodesManager, commands_parser: CommandsParser, result_callbacks: Optional[Dict[str, Callable]] = ..., cluster_response_callbacks: Optional[Dict[str, Callable]] = ..., startup_nodes: Optional[List[ClusterNode]] = ..., read_from_replicas: bool = ..., load_balancing_strategy: Optional[LoadBalancingStrategy] = ..., cluster_error_retry_attempts: int = ..., reinitialize_steps: int = ..., retry: Optional[Retry] = ..., lock=..., transaction=..., **kwargs) -> None:
        """ """
        ...
    
    def __repr__(self): # -> str:
        """ """
        ...
    
    def __enter__(self): # -> Self:
        """ """
        ...
    
    def __exit__(self, exc_type, exc_value, traceback): # -> None:
        """ """
        ...
    
    def __del__(self): # -> None:
        ...
    
    def __len__(self): # -> int:
        """ """
        ...
    
    def __bool__(self): # -> Literal[True]:
        "Pipeline instances should  always evaluate to True on Python 3+"
        ...
    
    def execute_command(self, *args, **kwargs): # -> None:
        """
        Wrapper function for pipeline_execute_command
        """
        ...
    
    def pipeline_execute_command(self, *args, **options): # -> None:
        """
        Stage a command to be executed when execute() is next called

        Returns the current Pipeline object back so commands can be
        chained together, such as:

        pipe = pipe.set('foo', 'bar').incr('baz').decr('bang')

        At some other point, you can then run: pipe.execute(),
        which will execute all commands queued in the pipe.
        """
        ...
    
    def annotate_exception(self, exception, number, command): # -> None:
        """
        Provides extra context to the exception prior to it being handled
        """
        ...
    
    def execute(self, raise_on_error: bool = ...) -> List[Any]:
        """
        Execute all the commands in the current pipeline
        """
        ...
    
    def reset(self): # -> None:
        """
        Reset back to empty pipeline.
        """
        ...
    
    def send_cluster_commands(self, stack, raise_on_error=..., allow_redirections=...): # -> None:
        ...
    
    def exists(self, *keys): # -> None:
        ...
    
    def eval(self): # -> None:
        """ """
        ...
    
    def multi(self): # -> None:
        """
        Start a transactional block of the pipeline after WATCH commands
        are issued. End the transactional block with `execute`.
        """
        ...
    
    def load_scripts(self): # -> None:
        """ """
        ...
    
    def discard(self): # -> None:
        """ """
        ...
    
    def watch(self, *names): # -> None:
        """Watches the values at keys ``names``"""
        ...
    
    def unwatch(self): # -> None:
        """Unwatches all previously specified keys"""
        ...
    
    def script_load_for_pipeline(self, *args, **kwargs): # -> None:
        ...
    
    def delete(self, *names): # -> None:
        ...
    
    def unlink(self, *names): # -> None:
        ...
    


def block_pipeline_command(name: str) -> Callable[..., Any]:
    """
    Prints error because some pipelined commands should
    be blocked when running in cluster-mode
    """
    ...

PIPELINE_BLOCKED_COMMANDS = ...
class PipelineCommand:
    """ """
    def __init__(self, args, options=..., position=...) -> None:
        ...
    


class NodeCommands:
    """ """
    def __init__(self, parse_response, connection_pool, connection) -> None:
        """ """
        ...
    
    def append(self, c): # -> None:
        """ """
        ...
    
    def write(self): # -> None:
        """
        Code borrowed from Redis so it can be fixed
        """
        ...
    
    def read(self): # -> None:
        """ """
        ...
    


class ExecutionStrategy(ABC):
    @property
    @abstractmethod
    def command_queue(self): # -> None:
        ...
    
    @abstractmethod
    def execute_command(self, *args, **kwargs): # -> None:
        """
        Execution flow for current execution strategy.

        See: ClusterPipeline.execute_command()
        """
        ...
    
    @abstractmethod
    def annotate_exception(self, exception, number, command): # -> None:
        """
        Annotate exception according to current execution strategy.

        See: ClusterPipeline.annotate_exception()
        """
        ...
    
    @abstractmethod
    def pipeline_execute_command(self, *args, **options): # -> None:
        """
        Pipeline execution flow for current execution strategy.

        See: ClusterPipeline.pipeline_execute_command()
        """
        ...
    
    @abstractmethod
    def execute(self, raise_on_error: bool = ...) -> List[Any]:
        """
        Executes current execution strategy.

        See: ClusterPipeline.execute()
        """
        ...
    
    @abstractmethod
    def send_cluster_commands(self, stack, raise_on_error=..., allow_redirections=...): # -> None:
        """
        Sends commands according to current execution strategy.

        See: ClusterPipeline.send_cluster_commands()
        """
        ...
    
    @abstractmethod
    def reset(self): # -> None:
        """
        Resets current execution strategy.

        See: ClusterPipeline.reset()
        """
        ...
    
    @abstractmethod
    def exists(self, *keys): # -> None:
        ...
    
    @abstractmethod
    def eval(self): # -> None:
        ...
    
    @abstractmethod
    def multi(self): # -> None:
        """
        Starts transactional context.

        See: ClusterPipeline.multi()
        """
        ...
    
    @abstractmethod
    def load_scripts(self): # -> None:
        ...
    
    @abstractmethod
    def watch(self, *names): # -> None:
        ...
    
    @abstractmethod
    def unwatch(self): # -> None:
        """
        Unwatches all previously specified keys

        See: ClusterPipeline.unwatch()
        """
        ...
    
    @abstractmethod
    def script_load_for_pipeline(self, *args, **kwargs): # -> None:
        ...
    
    @abstractmethod
    def delete(self, *names): # -> None:
        """
        "Delete a key specified by ``names``"

        See: ClusterPipeline.delete()
        """
        ...
    
    @abstractmethod
    def unlink(self, *names): # -> None:
        """
        "Unlink a key specified by ``names``"

        See: ClusterPipeline.unlink()
        """
        ...
    
    @abstractmethod
    def discard(self): # -> None:
        ...
    


class AbstractStrategy(ExecutionStrategy):
    def __init__(self, pipe: ClusterPipeline) -> None:
        ...
    
    @property
    def command_queue(self): # -> List[PipelineCommand]:
        ...
    
    @command_queue.setter
    def command_queue(self, queue: List[PipelineCommand]): # -> None:
        ...
    
    @abstractmethod
    def execute_command(self, *args, **kwargs): # -> None:
        ...
    
    def pipeline_execute_command(self, *args, **options): # -> ClusterPipeline:
        ...
    
    @abstractmethod
    def execute(self, raise_on_error: bool = ...) -> List[Any]:
        ...
    
    @abstractmethod
    def send_cluster_commands(self, stack, raise_on_error=..., allow_redirections=...): # -> None:
        ...
    
    @abstractmethod
    def reset(self): # -> None:
        ...
    
    def exists(self, *keys): # -> None:
        ...
    
    def eval(self):
        """ """
        ...
    
    def load_scripts(self):
        """ """
        ...
    
    def script_load_for_pipeline(self, *args, **kwargs):
        """ """
        ...
    
    def annotate_exception(self, exception, number, command): # -> None:
        """
        Provides extra context to the exception prior to it being handled
        """
        ...
    


class PipelineStrategy(AbstractStrategy):
    def __init__(self, pipe: ClusterPipeline) -> None:
        ...
    
    def execute_command(self, *args, **kwargs): # -> ClusterPipeline:
        ...
    
    def execute(self, raise_on_error: bool = ...) -> List[Any]:
        ...
    
    def reset(self): # -> None:
        """
        Reset back to empty pipeline.
        """
        ...
    
    def send_cluster_commands(self, stack, raise_on_error=..., allow_redirections=...): # -> list[Any]:
        """
        Wrapper for RedisCluster.ERRORS_ALLOW_RETRY errors handling.

        If one of the retryable exceptions has been thrown we assume that:
         - connection_pool was disconnected
         - connection_pool was reseted
         - refereh_table_asap set to True

        It will try the number of times specified by
        the retries in config option "self.retry"
        which defaults to 3 unless manually configured.

        If it reaches the number of times, the command will
        raises ClusterDownException.
        """
        ...
    
    def multi(self):
        ...
    
    def discard(self):
        ...
    
    def watch(self, *names):
        ...
    
    def unwatch(self, *names):
        ...
    
    def delete(self, *names): # -> ClusterPipeline:
        ...
    
    def unlink(self, *names): # -> ClusterPipeline:
        ...
    


class TransactionStrategy(AbstractStrategy):
    NO_SLOTS_COMMANDS = ...
    IMMEDIATE_EXECUTE_COMMANDS = ...
    UNWATCH_COMMANDS = ...
    SLOT_REDIRECT_ERRORS = ...
    CONNECTION_ERRORS = ...
    def __init__(self, pipe: ClusterPipeline) -> None:
        ...
    
    def execute_command(self, *args, **kwargs): # -> ClusterPipeline:
        ...
    
    def execute(self, raise_on_error: bool = ...) -> List[Any]:
        ...
    
    def reset(self): # -> None:
        ...
    
    def send_cluster_commands(self, stack, raise_on_error=..., allow_redirections=...):
        ...
    
    def multi(self): # -> None:
        ...
    
    def watch(self, *names): # -> ClusterPipeline:
        ...
    
    def unwatch(self): # -> ClusterPipeline | Literal[True]:
        ...
    
    def discard(self): # -> None:
        ...
    
    def delete(self, *names): # -> ClusterPipeline:
        ...
    
    def unlink(self, *names): # -> ClusterPipeline:
        ...
    


