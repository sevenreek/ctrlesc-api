"""
This type stub file was generated by pyright.
"""

import redis
from ...asyncio.client import Pipeline as AsyncioPipeline
from .commands import AGGREGATE_CMD, AsyncSearchCommands, CONFIG_CMD, INFO_CMD, PROFILE_CMD, SEARCH_CMD, SPELLCHECK_CMD, SYNDUMP_CMD, SearchCommands

class Search(SearchCommands):
    """
    Create a client for talking to search.
    It abstracts the API of the module and lets you just use the engine.
    """
    class BatchIndexer:
        """
        A batch indexer allows you to automatically batch
        document indexing in pipelines, flushing it every N documents.
        """
        def __init__(self, client, chunk_size=...) -> None:
            ...
        
        def __del__(self): # -> None:
            ...
        
        def add_document(self, doc_id, nosave=..., score=..., payload=..., replace=..., partial=..., no_create=..., **fields): # -> None:
            """
            Add a document to the batch query
            """
            ...
        
        def add_document_hash(self, doc_id, score=..., replace=...): # -> None:
            """
            Add a hash to the batch query
            """
            ...
        
        def commit(self): # -> None:
            """
            Manually commit and flush the batch indexing query
            """
            ...
        
    
    
    def __init__(self, client, index_name=...) -> None:
        """
        Create a new Client for the given index_name.
        The default name is `idx`

        If conn is not None, we employ an already existing redis connection
        """
        ...
    
    def pipeline(self, transaction=..., shard_hint=...): # -> Pipeline:
        """Creates a pipeline for the SEARCH module, that can be used for executing
        SEARCH commands, as well as classic core commands.
        """
        ...
    


class AsyncSearch(Search, AsyncSearchCommands):
    class BatchIndexer(Search.BatchIndexer):
        """
        A batch indexer allows you to automatically batch
        document indexing in pipelines, flushing it every N documents.
        """
        async def add_document(self, doc_id, nosave=..., score=..., payload=..., replace=..., partial=..., no_create=..., **fields): # -> None:
            """
            Add a document to the batch query
            """
            ...
        
        async def commit(self): # -> None:
            """
            Manually commit and flush the batch indexing query
            """
            ...
        
    
    
    def pipeline(self, transaction=..., shard_hint=...): # -> AsyncPipeline:
        """Creates a pipeline for the SEARCH module, that can be used for executing
        SEARCH commands, as well as classic core commands.
        """
        ...
    


class Pipeline(SearchCommands, redis.client.Pipeline):
    """Pipeline for the module."""
    ...


class AsyncPipeline(AsyncSearchCommands, AsyncioPipeline, Pipeline):
    """AsyncPipeline for the module."""
    ...


